{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba63988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from urllib.parse import urlencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cace87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch data from Dati Lombardia API\n",
    "def fetch_data_from_api(api_url, limit=1000, order=\"Data DESC\"):\n",
    "    \"\"\"\n",
    "    Fetch data from the API with specified limit and order\n",
    "    \n",
    "    Parameters:\n",
    "    - api_url: Base URL for the API\n",
    "    - limit: Number of records to return (default: 1000)\n",
    "    - order: Field and direction to sort by (default: \"datastop DESC\" for latest records)\n",
    "    \"\"\"\n",
    "    # Construct query parameters\n",
    "    params = {\n",
    "        \"$limit\": limit,\n",
    "        \"$order\": order\n",
    "    }\n",
    "    \n",
    "    # Append parameters to URL\n",
    "    full_url = f\"{api_url}?{urlencode(params)}\"\n",
    "    print(f\"Requesting data from: {full_url}\")\n",
    "    \n",
    "    response = requests.get(full_url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e7e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Process API data and filter out problematic columns\n",
    "def process_api_data(data_list):\n",
    "    processed_data = []\n",
    "    \n",
    "    # List of columns to exclude\n",
    "    columns_to_exclude = [\":@computed_region_6hky_swhk\"]\n",
    "    \n",
    "    for item in data_list:\n",
    "        processed_item = {}\n",
    "        \n",
    "        # Copy only the desired fields, skipping problematic ones\n",
    "        for key, value in item.items():\n",
    "            # Skip excluded columns\n",
    "            if key in columns_to_exclude:\n",
    "                continue\n",
    "                \n",
    "            # Handle nested dictionaries by converting to JSON strings\n",
    "            if isinstance(value, dict):\n",
    "                processed_item[key] = json.dumps(value)\n",
    "            else:\n",
    "                processed_item[key] = value\n",
    "                \n",
    "        processed_data.append(processed_item)\n",
    "        \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Connect to PostgreSQL database\n",
    "def connect_to_postgres():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",       \n",
    "        database=\"lombardia_air_quality\", \n",
    "        user=\"airdata_user\",    \n",
    "        password=\"user\"\n",
    "    )\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8df6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create table if it doesn't exist\n",
    "def create_table_if_not_exists(conn, table_name, data_sample):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create schema SQL statement based on the data structure\n",
    "    columns = []\n",
    "    for key, value in data_sample.items():\n",
    "        column_type = \"TEXT\"  # Default type\n",
    "        \n",
    "        # Try to infer the data type\n",
    "        if isinstance(value, int):\n",
    "            column_type = \"INTEGER\"\n",
    "        elif isinstance(value, float):\n",
    "            column_type = \"NUMERIC\"\n",
    "        elif isinstance(value, bool):\n",
    "            column_type = \"BOOLEAN\"\n",
    "        elif isinstance(value, dict):\n",
    "            column_type = \"JSONB\"  # Use JSONB for nested structures\n",
    "        # Special case for timestamp fields\n",
    "        elif key == \"datastart\" or key == \"datastop\":\n",
    "            column_type = \"TIMESTAMP\"\n",
    "\n",
    "        columns.append(f\"\\\"{key}\\\" {column_type}\")\n",
    "\n",
    "    create_table_sql = f\"\"\"\n",
    "    DROP TABLE IF EXISTS {table_name};\n",
    "    CREATE TABLE {table_name} (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        {', '.join(columns)},\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3db4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Insert data into table\n",
    "def insert_data(conn, table_name, data_list):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    if not data_list:\n",
    "        print(\"No data to insert\")\n",
    "        return\n",
    "    \n",
    "    # Get column names from the first data item\n",
    "    columns = list(data_list[0].keys())\n",
    "    \n",
    "    # Prepare values for insertion\n",
    "    values = [[item.get(col) for col in columns] for item in data_list]\n",
    "    \n",
    "    # Create the SQL query\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} ({', '.join([f'\"{col}\"' for col in columns])})\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query with all values\n",
    "    execute_values(cursor, insert_query, values)\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"Inserted {len(data_list)} records into {table_name}\")\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be15ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching latest 1000 records from API...\n",
      "Requesting data from: https://www.dati.lombardia.it/resource/g2hp-ar79.json?%24limit=1000&%24order=Data+DESC\n",
      "Received 1000 records from API\n",
      "Sample data item structure:\n",
      "{\n",
      "  \"idsensore\": \"30166\",\n",
      "  \"data\": \"2025-01-01T00:00:00.000\",\n",
      "  \"valore\": \"2.5\",\n",
      "  \"stato\": \"VA\",\n",
      "  \"idoperatore\": \"1\"\n",
      "}\n",
      "Processing data...\n",
      "Connecting to PostgreSQL...\n",
      "Creating table if it doesn't exist...\n",
      "Inserting data into table...\n",
      "Inserted 1000 records into station\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# API URL\n",
    "api_url = \"https://www.dati.lombardia.it/resource/g2hp-ar79.json\"\n",
    "\n",
    "# Define the table name for your data\n",
    "table_name = \"station\"\n",
    "\n",
    "try:\n",
    "    # Fetch data from API (latest 1000 records)\n",
    "    print(\"Fetching latest 1000 records from API...\")\n",
    "    raw_data = fetch_data_from_api(api_url, limit=1000, order=\"Data DESC\")\n",
    "    \n",
    "    # Debug: Inspect the data structure and count\n",
    "    print(f\"Received {len(raw_data)} records from API\")\n",
    "    print(\"Sample data item structure:\")\n",
    "    if raw_data:\n",
    "        print(json.dumps(raw_data[0], indent=2))\n",
    "    \n",
    "    # Process the data to handle nested structures and filter out problematic columns\n",
    "    print(\"Processing data...\")\n",
    "    processed_data = process_api_data(raw_data)\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    print(\"Connecting to PostgreSQL...\")\n",
    "    conn = connect_to_postgres()\n",
    "    \n",
    "    # Create table if it doesn't exist (using actual data to infer schema)\n",
    "    print(\"Creating table if it doesn't exist...\")\n",
    "    if processed_data:\n",
    "        create_table_if_not_exists(conn, table_name, processed_data[0])\n",
    "    \n",
    "    # Insert data into table\n",
    "    print(\"Inserting data into table...\")\n",
    "    insert_data(conn, table_name, processed_data)\n",
    "    \n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    print(\"Process completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se4g",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
